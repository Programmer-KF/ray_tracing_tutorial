# 光线追踪——从入门到放弃(1)：光线追踪是个啥
## 引言
上图是各位中学时都喜闻乐见的光路图，从某种角度上看，它也是一个精简版的光路追踪了。

## 光线追踪是个啥
通俗理解，光线追踪的原理就跟它的名字一样，我们去模拟物理中的光线，光线是怎么走的，我们就跟着它走。具体来讲，一条光线从光源处出发，途中条光线可能会击中某个物体，然后就会发生折射或反射，并且会损失一部分光能。那些最终能射入我们的眼睛（或照相机）的光线，就会在我们的眼睛（或照相机）内留下影像，我们就看到了光来源位置的物体。而我们的渲染器就是去模拟一个这样的过程。（有一点需要注意的是，在目前主流的光线追踪渲染器中，我们都只考虑光的粒子性，忽略光的波的性质。这意味这我们大多数时候忽略光的色散，基本不考虑光的干涉与衍射。当然，在比较前沿的研究中，的确有在光线追踪中研究光的波的性质的，但因为其过于前沿，我们不去考虑。）

上一张示意图：


但是!!!如果我们只是这样模拟，会出现一个很大的问题：我们的照相机模型只是一个点（模型示意图见下文），而我们的光线想要击中这样一个点的概率为0！那有没有办法解决这样一个问题呢。答案显然是肯定的。我们在小学就学过光路可逆原理，所以如果我们将所有光线反向，是不会影响正确性的，这样就变成了光从照像机发出射向光源，概率比之前大多了。因此，我们的算法变成了：从眼睛中投射出一条射线->这条击中物体后进行折射或反射->这条光线击中光源->获取光源的信息（颜色与亮度）->回溯以计算亮度的损失->最终在光屏上着色。

再上一张示意图：


了解完光线追踪要干啥之后，我们就大概知道我们需要一些什么东西了：
1. 一个坐标系
2. 一个照相机
3. 光线
4. 光线的折射与反射
5. 物体
6. 光源
7. 其他杂七杂八的东西

我们这一节主要的任务就是初步实现1，2，3。剩下的之后再说

## 坐标系

在图像学中，我们有好多的坐标系，我们这里讲我们可能需要用到的三个：
1. 世界坐标系：就是对我们的场景进行建系，用于定义物体的位置
2. 物体坐标系：每个物体都有的自己特有的坐标系。不同物体之间的坐标系相互独立，没有任何联系。同时，如果物体发生移动或者旋转，物体坐标系发生相同的平移或者旋转。
3. 摄像机坐标系：以照相机为原点的坐标系，跟随照相机进行移动与转动

上一张示意图：


在三维中，我们的世界坐标系长这样：

我们的摄像机坐标系长这样：

X,Y轴这样设立是为了与生成的图片中的X,Y轴重合（或平行），其中Z轴之所以往（摄像机）后是要保持整个坐标系为右手系。


## 光线

一条光线（这里的“光线”严格来讲因该是视线，因为我们在上文提到了，我们的“光线”是由眼睛发出的射线，因此，我们的“光线”没有颜色的属性，颜色的相关计算单独处理）有两个属性：起点和方向，我们可以通过$\mathbf p=\mathbf s+t \cdot \mathbf d \tag{ray}$来表示光线上的一点（加粗代表向量），其中$\mathbf p$表示光线上某一点的坐标，$\mathbf s$表示光线起点的坐标，$\mathbf d$表示光线的方向向量。

在《RAY TRACING IN ONE WEEKEND》这本书中，作者表示"Note that I do not make the ray direction a unit length vector because I think not doing that makes for simpler and slightly faster code."(我认为如果我不把方向向量转换为单位向量的话，我的代码将会更加简洁并且跑得更快)，但是我曾经因为没有把方向向量单位化，导致出现了一些bug，所以，在我的代码中，我仍然将光线的方向向量转换为了单位向量。

于是我们就可以写下我们的光线类了（伪代码如下）

```c++
class ray
{
	public:
		ray(const vec3 &origin,const vec3 &direction):origin_(origin),direction_(direction.normalized()){}
		ray(const ray &r):origin_(r.origin()),direction_(r.direction()){};
		const vec3& origin()const{return origin_;}
		const vec3& direction()const{return direction_;}
		vec3 go(const real &t)const{return origin_+direction_*t;}
	private:
		vec3 origin_,direction_;
};
```

## 照相机
上一张示意图：


我们这里使用的相机和人眼类似，使用的是透视投影。图中黑点代表相机（视点），网格代表我们要生成的图像，图中每一小格代表一个像素。我们照相机的工作就是：我们指定图片中要采样的点（如图中的绿点），然后照相机就把我们指定的那个点生成一条光线（如图中红色的射线），并返回这条光线的世界坐标。


我们今天只讲简单的照相机，也就是把相机放在世界坐标系原点，相机朝向世界坐标系的X轴正方向（相机坐标系中的Z轴正方向指向世界坐标系的X轴正方向），图片长宽比固定为2：1（更好的照相机我们以后再说）。（如图，绿色代表世界坐标系，红色代表图像中的坐标系，橙色代表相机坐标系，黑色坐标是在世界坐标系中的坐标

所以，我们这么实现这个照相机呢？假设我们要采样的是图中的P点，我们要生成的就是起点在O点，方向为$\vec{OP}$的光线。而$\vec{OP}=\vec{OF}+\vec{FP}$,我们已经知道$\vec{OF}=(1,0,0)$。所以我们只需解决$\vec{FP}$即可。而F,P都在图像平面内，所以如果我们知道决F点在图像平面内红色坐标系的坐标，比如说为(n,m)，那我们$\vec{FP}=(0,-n,m)$，而$\vec{OP}=(1,-n,m)$。接下来，我们来求F点在图像平面内红色坐标系的坐标，考虑图像平面：


一般来讲，我们指定P点坐标是用第i行，第j个像素来表示的（如图中黑色坐标系），但我们最终需要的是用红色坐标系所使用的坐标，因此，我们需要将黑色坐标转换为红色坐标。我们可以通过蓝色的坐标系过度（黑色坐标系与蓝色坐标系坐标轴重合，但单位长度不同；蓝色坐标系与红色坐标系单位长度相同但原点位置和坐标轴方向不同）。

黑色坐标转蓝色坐标：x轴方向，图像在黑色坐标的长度为400（记为height）个像素，而图像在蓝色坐标系的x轴长度为1，所以第i行的像素对应的蓝色x轴坐标为$1 \times \frac{i}{height}$；同理，y轴方向，图像在黑色坐标的长度为800（记为width）个像素，而图像在蓝色坐标系的y轴长度为2，所以第j列的像素对应的蓝色y轴坐标为$2 \times \frac{j}{width}$。

蓝色坐标转红色坐标：$\vec{FP}=\vec{FA}+\vec{AP}$，而$\vec{FP}=(-1,0.5)$，若$\vec{AP}$在蓝色坐标系中为(u,v)的话，它在红色坐标系中会为(v,-u)。因此，P点在红色坐标系下坐标为(-1+v,0.5-u)。

综上，我们可以求得$\vec{OP}=(1,2 \times \frac{j}{width}-1,0.5-\frac{i}{height})$，一般来讲，我们将指定点的坐标传给照相机时，不是直接将像素行列（也就是公式中的i和j）传给照相机，而是直接传入$\frac{j}{width}$和$\frac{i}{height}$，也就是归一化后的坐标，这样之后，不管我们的图片是$200 \times 400$还是$400 \times 800$，我们的相机都不care，只要是1：2他都能直接处理。

	
```c++
class camera
{
	public:
		ray get_ray(real u,real v){return ray(vec3(0,0,0),vec3(1,2*v-1,0.5-u));}
		ray get_ray(vec2 p){return ray(vec3(0,0,0),vec3(1,2*p.y()-1,0.5-p.x()));};
}
```

## 测试

最后我们来测试一下我们的代码：

```c++
#include<png.h>
#include<cstdio>
#include<cstdlib>
#include<iostream>
#include<Eigen/Eigen>
#include<Eigen/Dense>

#include<core.h>
using namespace Eigen;
using namespace std;
using namespace vec;
using namespace ray_tracing;
const int width=400;//图像水平长度/像素
const int height=200;//图像竖直高度/像素
const int pixel_size=3;//通道数(RGB为3,RGBA为4)
const char *file_name="hello.png";//图片名称
vec3 bitmap_rgb[height][width];//范围为[0,1]
unsigned char bitmap[height][width*pixel_size];
void write_PNG()//输出图像
{
	png_structp png_ptr=png_create_write_struct(PNG_LIBPNG_VER_STRING,nullptr,nullptr,nullptr);
	png_infop info_ptr=png_create_info_struct(png_ptr);
	FILE* f=fopen(file_name,"wb");
	png_init_io(png_ptr,f);
	png_set_IHDR(png_ptr,info_ptr,width,height,8,pixel_size==3?PNG_COLOR_TYPE_RGB:PNG_COLOR_TYPE_RGBA,PNG_INTERLACE_NONE,PNG_COMPRESSION_TYPE_BASE,PNG_FILTER_TYPE_BASE);
	png_set_packing(png_ptr);
	png_write_info(png_ptr,info_ptr);

	for(int i=0;i<height;i++)
	for(int j=0;j<width;j++)
	for(int k=0;k<pixel_size;k++)bitmap[i][j*pixel_size+k]=(int)(bitmap_rgb[i][j][k]*255);

	png_bytepp rows=(png_bytepp)png_malloc(png_ptr,height*sizeof(png_bytep));
	for(int i=0;i<height;i++)rows[i]=(png_bytep)bitmap[i];

	png_write_image(png_ptr,rows);
	png_write_end(png_ptr,info_ptr);
	png_free(png_ptr,rows);//cleanup
	png_destroy_write_struct(&png_ptr,&info_ptr);
	fclose(f);
}
int main()
{
	camera cam;
	for(int i=0;i<height;i++)
	for(int j=0;j<width;j++)
	{
		vec3 d=cam.get_ray((vec::real)i/height/*将坐标归一化*/,(vec::real)j/width/*将坐标归一化*/).direction();
		vec2 f(d.y(),d.z());
		f=f.normalized();
		vec3 g(0,(f.x()+1)/2,(f.y()+1)/2);
		bitmap_rgb[i][j]=g;
	}
	write_PNG();
	return 0;
}
```

输出：

看上去没有什么问题。